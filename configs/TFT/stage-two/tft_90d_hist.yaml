seed_everything: 42

# ---------------------------- TRAINER -------------------------------------------
trainer:
  default_root_dir: /glade/derecho/scratch/kvirji/mjo-predict/exps/TFT/stage-two/90d_hist
  precision: 32

  devices: 1
  num_nodes: 1
  accelerator: gpu
  strategy: auto     
    # class_path: pytorch_lightning.strategies.DDPStrategy
    # init_args:
    #   timeout: 02:00:00
      # find_unused_parameters: true

  min_epochs: 1
  max_epochs: 250
  enable_progress_bar: true

  sync_batchnorm: false
  enable_checkpointing: true

  gradient_clip_val: 1.0

  # debugging
  fast_dev_run: false

  logger:
    class_path: pytorch_lightning.loggers.tensorboard.TensorBoardLogger
    init_args:
      save_dir: ${trainer.default_root_dir}/logs
      name: null
      version: null
      log_graph: false
      default_hp_metric: true
      prefix: ""

  callbacks:
    # - class_path: pytorch_lightning.callbacks.LearningRateFinder
    #   init_args:
    #     attr_name: "lr"

    - class_path: pytorch_lightning.callbacks.LearningRateMonitor
      init_args:
        logging_interval: "step"

    - class_path: mjo.utils.callbacks.GradientMonitor
      init_args:
        log_every_n_steps: 50

    - class_path: pytorch_lightning.callbacks.ModelCheckpoint
      init_args:
        dirpath: "${trainer.default_root_dir}/checkpoints"
        monitor: "val/mse" # name of the logged metric which determines when model is improving
        mode: "min" # "max" means higher metric value is better, can be also "min"
        save_top_k: 1 # save k best models (determined by above metric)
        save_last: true # additionaly always save model from last epoch
        verbose: false
        filename: "epoch_{epoch:03d}"
        auto_insert_metric_name: false

    - class_path: pytorch_lightning.callbacks.EarlyStopping
      init_args:
        monitor: "val/mse" # name of the logged metric which determines when model is improving
        mode: "min" # "max" means higher metric value is better, can be also "min"
        patience: 10 # how many validation epochs of not improving until training stops
        min_delta: 0. # minimum change in the monitored metric needed to qualify as an improvement

    - class_path: pytorch_lightning.callbacks.RichModelSummary
      init_args:
        max_depth: -1

    - class_path: pytorch_lightning.callbacks.RichProgressBar

# ---------------------------- MODEL -------------------------------------------
model:
  pretrained_path: "/glade/derecho/scratch/kvirji/mjo-predict/exps/TFT/stage-one/90d_hist/checkpoints/epoch_066.ckpt"
  date_variables: ['year', 'doy_sin', 'doy_cos']
  known_future_variables: ['RMM1_forecast', 'RMM2_forecast', 'forecast_mask']
  num_static_components: 0
  hidden_size: 128
  lstm_layers: 3
  num_attention_heads: 4
  full_attention: false
  feed_forward: "GatedResidualNetwork"
  hidden_continuous_size: 128
  add_relative_index: false
  dropout: 0.1
  norm_type: "LayerNorm"
  lr: 3.9e-6
  beta_1: 0.9
  beta_2: 0.99
  weight_decay: 8.1e-5
  warmup_steps: 1000
  max_steps: 50000
  save_outputs: true

# ---------------------------- DATA -------------------------------------------
data:
  root_dir: /glade/derecho/scratch/kvirji/DATA/MJO/U250/preprocessed
  forecast_dir: /glade/derecho/scratch/kvirji/DATA/MJO/U250/preprocessed/FuXi
  load_forecast_members: false
  load_forecast_samples: true
  forecast_length: 42
  ensemble_members: 1
  in_variables:
    - "RMM1"
    - "RMM2"
  out_variables:
    - "RMM1"
    - "RMM2"
  predictions: [1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60]
  history: [-90,-89,-88,-87,-86,-85,-84,-83,-82,-81,-80,-79,-78,-77,-76,-75,-74,-73,-72,-71,-70,-69,-68,-67,-66,-65,-64,-63,-62,-61,-60,-59,-58,-57,-56,-55,-54,-53,-52,-51,-50,-49,-48,-47,-46,-45,-44,-43,-42,-41,-40,-39,-38,-37,-36,-35,-34,-33,-32,-31,-30,-29,-28,-27,-26,-25,-24,-23,-22,-21,-20,-19,-18,-17,-16,-15,-14,-13,-12,-11,-10,-9,-8,-7,-6,-5,-4,-3,-2,-1]
  filter_mjo_events: false
  normalize_data: true
  max_buffer_size: 10000
  batch_size: 64
  num_workers: 0
  pin_memory: true